---
title: "(WIP) Boundary Problems - Refining Solutions"
format: html
---


## Evolving a solution

The sheer number of possible solutions makes this a tricky problem to solve!

We could keep generating a very large number of random solutions, and this may work to eventually find a near-optimal solution, but could be extremely time-consuming and resource-intensive, requiring many hours of compute power.

Instead, let's try taking some inspiration from the natural world and **evolving** our solutions until they are as good as can be.

:::{.callout-tip}
This is a big part of the field of operational research, with certain algorithms like NSGA-II being well-known and defined, and implemented in libraries that you can reuse. However, you can also create your own genetic algorithms or modify approaches like NSGA-II, as done in [Allen et al. and their work on hyperacute stroke unit modelling](https://github.com/MichaelAllen1966/stroke_unit_location/blob/master/bmj_open_paper/bmj_open_stroke_unit_location_paper_supplementary_material-1.pdf).
:::

:::{.callout-note}
Due to the requirements for solutions to follow certain rules, evolving our solutions is somewhat more complex than in the case of our location allocation problem, where any combination of centres was theoretically 'valid'. In that instance, we could randomly permute or mix solutions with no concerns surrounding what the output looked like beyond not being an exact duplicate of an already-evaluated solution. Here, with a need to generate a solution that conforms to rules like the dispatcher's patch being continuous and all regions being allocated to one (and only one) dispatcher.
:::

So what will the process look like in this case?

- We will generate a new random solution using our function from the previous section
- We will evaluate that solution
    - While we will initially be evaluating this solution against a single metric, the same approach could be applied when scoring against multiple metrics
- If this solution is worse than the current status quo, we will generate a new random solution (and keep doing so until we find a solution that performs 'better' than the current status quo)
- If this solution is better than the current status quo, we will then start to evolve this solution
    - for each 'player' (in this case, our dispatchers), we will randomly add or remove a small number of their regions and allocate it to one of the other players for whom it would be valid territory (i.e. they share a boundary with the territory in question)
        - we will generate many of these variants per round
        - we will then assess the performance of each of these solutions
        - several of the best solutions (a parameter we will be able to vary) will be retained at this point, going on to form a new population of solutions we can continue to loop through, tweak, and recheck the performance of
            - rather than the 'child' solutions replacing the 'parents', we will keep both at each stage, evaluating them all on their merits rather than always assuming the 'child' strategies will be better
            - note that general recommendations are that ùù∫ - the population size - should be at least 10 times larger than Œº - the number of solutions we keep in each generation
            - it's also recommended that ùù∫ / Œº should leave no remainder
    - we will continue this process until there has been no improvement for several rounds of attempts
        - the number of rounds for which there is no improvement will be a parameter we can vary
- We will then store the performance of this solution, then start with a new random solution and repeat this for a certain number of solutions or period of time

:::{.callout-tip}
As the minor variation and evaluation of solutions is much quicker than generating brand new solutions with the method we developed in the previous chapter, we should see much faster movement towards a 'good' solution. This approach should therefore be more efficient than if we were to use the same amount of computational time generating and evaluating random solutions (though there is a small chance we land on a great solution at random!).

However, it should also be remembered that there is something to be said for a simple approach...

> ‚ÄúExperience shows that if the stakeholders (users of the model) can easily understand the methods used, they are more likely to trust the solution and use the model more confidently in the decision making processes‚Äù - Meskarian R, Penn ML, Williams S, Monks T (2017)[^trustquote]

:::

[^trustquote]: Meskarian R, Penn ML, Williams S, Monks T (2017) A facility location model for analysis of current and future demand for sexual health services. PLoS ONE 12(8): e0183942. https://doi.org/10.1371/journal.pone.0183942


## Local and Global Optima

![](assets/2025-02-11-10-35-43.png)[^imglocaloptima]

[^imglocaloptima]: A genetic algorithm for calculating minimum distance between convex and concave bodies - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/figure/Local-versus-global-optimum_fig1_228563694 [accessed 9 Apr, 2024]

Evolutionary algorithms can have the issue of getting stuck in local optima - where they reach a 'good' solution and small tweaks in any direction lead to worse scores. However, outside of the 'sight' of the algorithm, there may be a better overall solution that exists.

While there are a range of ways this issue can be tackled, here we'll mainly be approaching it by working with several different random starting positions.

## Loading Previous Solutions

To start with, let's load our random solutions from the last chapter back in.

:::{.callout-warning}
Pickle is a convenient file format for storing dataframes and reloading them with important information like column types retained. This is important in situations like this where one of our columns is a more complex datatype that may not transfer well into a csv.

However, pickle is not particularly secure and you should only unpickle files from sources you trust as there is a risk of malicious code being injected and then executing on unpickling. Alternative formats such as [Feather](https://arrow.apache.org/docs/python/feather.html) and [Parquet](https://arrow.apache.org/docs/python/parquet.html) are generally regarded as more secure.
:::

```{python}
#| label: read-solutions-example

import pandas as pd

initial_solution_df = pd.read_pickle('solutions_example.pkl')

initial_solution_df.to_csv('solutions_example.csv')
```

Let's write a quick function to pull out the allocations from our solutions file, and turn that back into a dataframe we can work with.

```{python}
#| label: define-extract-allocations-df-function

def extract_allocation_df_from_solution_df(df, solution_rank, allocation_col_name, territory_col_name):
    row_of_interest = df[df['rank'] == solution_rank]
    owned_territory_dict = row_of_interest['allocations'].values[0]

    owned_territory_df = pd.DataFrame(
            [(key, value) for key, values in owned_territory_dict.items() for value in values],
            columns=[allocation_col_name, territory_col_name])
    return owned_territory_df
```

Let's now try this out to pull back and visualise our best solution.

```{python}
#| label: extract-best-solution

best_solution = extract_allocation_df_from_solution_df(
    initial_solution_df,
    solution_rank=1,
    allocation_col_name="centre_dispatcher_NEW",
    territory_col_name="LSOA11CD"
    )

best_solution
```

:::{.callout-note collapse="true"}
### Click here to view code from previous chapters for importing our historical boundary and demand data

```{python}
#| label: import-historical-boundaries-demand-repeat-earlier-chapter

import geopandas

lsoa_geojson_path = 'https://github.com/hsma-programme/h6_3c_interactive_plots_travel/raw/main/h6_3c_interactive_plots_travel/example_code/LSOA_2011_Boundaries_Super_Generalised_Clipped_BSC_EW_V4.geojson'

lsoa_boundaries = geopandas.read_file(lsoa_geojson_path)

xmin, xmax = 370000, 420000
ymin, ymax = 250000, 310000

bham_region = lsoa_boundaries.cx[xmin:xmax, ymin:ymax]

bham_region["region"] = bham_region["LSOA11NM"].str[:-5]

boundary_allocations_df = pd.read_csv("boundary_allocations.csv")

bham_region = pd.merge(
    bham_region,
    boundary_allocations_df,
    left_on="region",
    right_on="Region",
    how="left"
)

bham_region["centre_dispatcher"] = bham_region["Centre"].astype("str") + '-' + bham_region["Dispatcher"].astype("str")

demand = pd.read_csv("demand_pop_bham.csv")

bham_region = bham_region.merge(demand, on="LSOA11CD")

# Create df of original boundaries
grouped_dispatcher_gdf = bham_region.groupby("centre_dispatcher")

# Create a new GeoDataFrame for the boundaries of each group
boundary_list = []

for group_name, group in grouped_dispatcher_gdf:
    # Combine the polygons in each group into one geometry
    combined_geometry = group.unary_union

    # Get the boundary of the combined geometry
    boundary = combined_geometry.boundary

    # Add the boundary geometry and the group name to the list
    boundary_list.append({'group': group_name, 'boundary': boundary})

# Create a GeoDataFrame from the list of boundaries
grouped_dispatcher_gdf_boundary = geopandas.GeoDataFrame(boundary_list, geometry='boundary', crs=bham_region.crs)

```
:::

```{python}
#| label: visualise-historical-boundaries-and-best-sol

ax=bham_region.merge(best_solution, on="LSOA11CD").plot(column="centre_dispatcher_NEW")

# Visualise the historical boundaries
grouped_dispatcher_gdf_boundary.plot(
    ax=ax,
    linewidth=2,
    edgecolor="black"
)
```

And let's compare this with our worst-performing solution.

:::{.callout-tip collapse="true"}
### Click here to see the code
```{python}
#| label: visualise-worst-performing-solution
#| eval: false
worst_solution = extract_allocation_df_from_solution_df(
    initial_solution_df,
    solution_rank=20,
    allocation_col_name="centre_dispatcher_NEW",
    territory_col_name="LSOA11CD"
    )

ax=bham_region.merge(worst_solution, on="LSOA11CD").plot(column="centre_dispatcher_NEW")

# Visualise the historical boundaries
grouped_dispatcher_gdf_boundary.plot(
    ax=ax,
    linewidth=2,
    edgecolor="black"
)
```
:::

```{python}
#| label: plot-worst-performing-solution
#| echo: false
#| eval: true
worst_solution = extract_allocation_df_from_solution_df(
    initial_solution_df,
    solution_rank=20,
    allocation_col_name="centre_dispatcher_NEW",
    territory_col_name="LSOA11CD"
    )

ax=bham_region.merge(worst_solution, on="LSOA11CD").plot(column="centre_dispatcher_NEW")

# Visualise the historical boundaries
grouped_dispatcher_gdf_boundary.plot(
    ax=ax,
    linewidth=2,
    edgecolor="black"
)
```

## Writing our evolutionary algorithm

:::{.callout-tip collapse="true"}
### Click here for a reminder of what we are trying to code in our evolutionary algorithm

- If this solution is better than the current status quo, we will then start to evolve this solution
    - for each 'player' (in this case, our dispatchers), we will randomly add or remove a small number of their regions and allocate it to one of the other players for whom it would be valid territory (i.e. they share a boundary with the territory in question)
        - we will generate many of these variants per round
        - we will then assess the performance of each of these solutions
        - several of the best solutions (a parameter we will be able to vary) will be retained at this point, going on to form a new population of solutions we can continue to loop through, tweak, and recheck the performance of
            - rather than the 'child' solutions replacing the 'parents', we will keep both at each stage, evaluating them all on their merits rather than always assuming the 'child' strategies will be better
            - note that general recommendations are that ùù∫ - the population size - should be at least 10 times larger than Œº - the number of solutions we keep in each generation
            - it's also recommended that ùù∫ / Œº should leave no remainder
    - we will continue this process until there has been no improvement for several rounds of attempts
        - the number of rounds for which there is no improvement will be a parameter we can vary
- We will then store the performance of this solution, then start with a new random solution and repeat this for a certain number of solutions or period of time
:::


We're also going to reuse our 'add_neighbours_column()' function from before.

:::{.callout-note collapse="true"}
### Click here to view the add_neighbours_column() function from the previous chapter
```{python}
#| label: define-add-neighbours-column-function

def add_neighbors_column(gdf):
    """
    Adds a column to the GeoDataFrame containing lists of indices of neighboring polygons
    based on the 'touches' method.
    """
    gdf = gdf.copy()
    neighbors = []
    for idx, geom in gdf.geometry.items():
        touching = gdf[gdf.geometry.touches(geom)]["LSOA11CD"].tolist()
        neighbors.append(touching)

    gdf["neighbors"] = neighbors
    return gdf

def find_border_dispatchers(row, df, allocation_colname='centre_dispatcher_NEW'):
    current_dispatcher = row[allocation_colname]
    neighbors = row['neighbors']

    # Get dispatchers of neighboring LSOAs
    neighboring_dispatchers = {
        df.loc[df['LSOA11CD'] == neighbor, allocation_colname].values[0]
        for neighbor in neighbors if not df[df['LSOA11CD'] == neighbor].empty
    }

    # Filter to only different dispatchers
    border_dispatchers = list(neighboring_dispatchers - {current_dispatcher})

    return border_dispatchers if border_dispatchers else []

```

:::


```{python}
#| label: define-permutation-functions

import random
from shapely.geometry import MultiPolygon

def is_solution_continuous(solution_gdf, allocation_col):
    """
    Uses spatial indexing to quickly check if dispatchers have disjoint regions.
    """

    # Dissolve regions by dispatcher
    dispatcher_groups = solution_gdf.dissolve(by=allocation_col)

    for geom in dispatcher_groups.geometry:
        if isinstance(geom, MultiPolygon):
            # Check if all parts of MultiPolygon touch each other
            parts = list(geom.geoms)  # Extract individual polygons
            for i in range(len(parts)):
                if not any(parts[i].touches(parts[j]) for j in range(len(parts)) if i != j):
                    return False  # Found a fully disconnected part

    return True

def is_continuous_after_swap(solution_gdf, lsoa,
                             current_dispatcher,
                             proposed_dispatcher,
                             allocation_col):
    """
    Checks if assigning 'lsoa' to 'new_dispatcher' maintains contiguous regions.
    This version avoids making a full DataFrame copy.
    """
    # Temporarily assign new dispatcher
    solution_gdf.loc[solution_gdf["LSOA11CD"] == lsoa, allocation_col] = proposed_dispatcher

    # Check continuity
    is_valid = is_solution_continuous(solution_gdf, allocation_col)

    # Revert change immediately (avoiding full copy overhead)
    solution_gdf.loc[solution_gdf["LSOA11CD"] == lsoa, allocation_col] = current_dispatcher

    return is_valid
    #return True

def assign_new_dispatcher(row,
                          solution_gdf,
                          border_colname,
                          permutation_chance_per_border,
                          new_allocation_colname
                          ):
        """
        Attempts to assign a new dispatcher while keeping the solution continuous.
        """

        # If no bordering dispatchers, keep the original
        if not row[border_colname]:
            #print(f'No borders in {row["LSOA11CD"]}')
            return row[new_allocation_colname]

        # If some bordering dispatchers, then randomly sample whether we will try to permute it
        elif random.uniform(0.0, 1.0) < permutation_chance_per_border:

            # Randomly select from border dispatchers
            current_dispatcher = row[new_allocation_colname]
            random_dispatcher = random.choice(row[border_colname])

            # Check if assigning this dispatcher keeps the solution contiguous
            if is_continuous_after_swap(solution_gdf,
                                        lsoa = row["LSOA11CD"],
                                        current_dispatcher=current_dispatcher,
                                        proposed_dispatcher=random_dispatcher,
                                        allocation_col=new_allocation_colname):
                #print(f"*** Swapped {row['LSOA11CD']} to {random_dispatcher} ***")
                return random_dispatcher
            else:
                #print(f":( Tried{row['LSOA11CD']} from {current_dispatcher} to {random_dispatcher}")
                return row[new_allocation_colname]  # Default to original allocation
        else:
            #print(f"Not trying to permute {row['LSOA11CD']}")
            return row[new_allocation_colname]

```

```{python}
#| label: define-initial-evolutionary-function

from joblib import Parallel, delayed

def create_evolved_solutions(
        initial_solution_df, geodataframe,
        join_col_left, join_col_right,
        original_allocation_colname='centre_dispatcher',
        random_solution_allocation_colname='centre_dispatcher_NEW',
        new_allocation_colname='centre_dispatcher_evolved',
        border_colname='border_dispatchers',
        permutation_chance_per_border=0.2,
        population_size=1
    ):
    """
    Generates evolved dispatcher allocations while ensuring that all regions remain contiguous.
    """

    # Merge initial solution with geodataframe to keep it as a geodataframe
    initial_solution_gdf = geodataframe.merge(initial_solution_df, left_on=join_col_left, right_on=join_col_right)

    # Compute neighbors and border dispatchers
    initial_solution_gdf = add_neighbors_column(initial_solution_gdf)
    initial_solution_gdf[border_colname] = initial_solution_gdf.apply(
        find_border_dispatchers, axis=1, df=initial_solution_gdf
        )

    # new_allocation_dfs = []

    # Include new_allocation_colname from the start
    simplified_allocation_df = (
        initial_solution_gdf[
            [random_solution_allocation_colname, 'LSOA11CD', border_colname, "geometry"]
            ].copy()
            )

    simplified_allocation_df[new_allocation_colname] = (
        simplified_allocation_df[random_solution_allocation_colname]
        )

    def process_iteration(i, simplified_allocation_df, border_colname,
                     permutation_chance_per_border, new_allocation_colname):

        # for i in range(population_size):
        evolved_df = simplified_allocation_df.copy(deep=True)

        # Separate out the cols with and without borders
        non_borders_df = evolved_df[evolved_df[border_colname].apply(len) == 0]
        borders_df = evolved_df[evolved_df[border_colname].apply(len) > 0]
        print(f"{len(non_borders_df)} regions without borders and {len(borders_df)} with borders")

        borders_df[new_allocation_colname] = borders_df.apply(
            lambda row: assign_new_dispatcher(
                row, borders_df,
                border_colname,
                permutation_chance_per_border,
                new_allocation_colname
                ), axis=1)

        return pd.concat([non_borders_df, borders_df], ignore_index=True)

        # evolved_df = pd.concat([non_borders_df, borders_df], ignore_index=True)

        #new_allocation_dfs.append(evolved_df.copy(deep=True))

        del evolved_df, non_borders_df, borders_df

    return Parallel(n_jobs=-1, verbose=10)(
            delayed(process_iteration)(
                i,
                simplified_allocation_df,
                border_colname,
                permutation_chance_per_border,
                new_allocation_colname
            )
            for i in range(population_size)
        )

```


```{python}
#| label: generate-one-solution
#| warning: false

solution = create_evolved_solutions(
    best_solution,
    bham_region,
    join_col_left="LSOA11CD",
    join_col_right="LSOA11CD",
    population_size=1
)

solution[0]

```

### Confirming solutions are valid

Let's now visualise this.

We will first create a df showing the boundaries of our plots in our original solution.

```{python}
#| label: get-starter-solution-boundaries

# Create df of original boundaries
grouped_dispatcher_gdf_starting_solution = bham_region.merge(best_solution, on="LSOA11CD").groupby("centre_dispatcher_NEW")

# Create a new GeoDataFrame for the boundaries of each group
boundary_list = []

for group_name, group in grouped_dispatcher_gdf_starting_solution:
    # Combine the polygons in each group into one geometry
    combined_geometry = group.unary_union

    # Get the boundary of the combined geometry
    boundary = combined_geometry.boundary

    # Add the boundary geometry and the group name to the list
    boundary_list.append({'group': group_name, 'boundary': boundary})

# Create a GeoDataFrame from the list of boundaries
grouped_dispatcher_gdf_starting_solution = geopandas.GeoDataFrame(boundary_list, geometry='boundary', crs=bham_region.crs)
```

Then we can plot the solution, comparing it with the boundaries prior to evolution.

```{python}
#| label: plot-single-solution

ax = solution[0].plot(column="centre_dispatcher_evolved", legend=True,
                      legend_kwds={'bbox_to_anchor': (1.4, 1)})

# Plot historical boundaries
grouped_dispatcher_gdf_starting_solution.plot(ax=ax, linewidth=1, edgecolor="black")

ax.axis("off")  # Hide axes for better visualization
```

### Generating multiple solutions at once

Let's try generating more.

We now see benefits to having a solution with multiple outputs.

```{python}
#| label: generate-thirty-solutions
#| warning: false

solution = create_evolved_solutions(
    best_solution,
    bham_region,
    join_col_left="LSOA11CD",
    join_col_right="LSOA11CD",
    population_size=30
)

solution[25]

```

### Confirming solutions are distinct

Let's confirm these solutions are distinct!

```{python}
#| label: check-distinct-solutions

def remove_duplicate_dataframes(df_list):
    """
    Removes duplicate dataframes from a list using hashing.

    Args:
        df_list (list of pd.DataFrame): List of pandas DataFrames.

    Returns:
        list of pd.DataFrame: List with duplicates removed.
    """
    seen_hashes = set()
    unique_dfs = []

    for df in df_list:
        df_hash = pd.util.hash_pandas_object(df[['LSOA11CD','centre_dispatcher_evolved']], index=True).values.tobytes()  # Ensure a hashable type
        if df_hash not in seen_hashes:
            seen_hashes.add(df_hash)
            unique_dfs.append(df)

    return unique_dfs

print(f"There are {len(solution)} solutions generated")

print(f"There are {len(remove_duplicate_dataframes(solution))} unique solutions generated")

```

### Plotting Multiple Solutions

Now let's plot 20 of the solutions to confirm they are valid.

```{python}
#| label: plot-mult-solutions
import matplotlib.pyplot as plt

fig, axes = plt.subplots(5, 4, figsize=(15, 15))  # Adjust figure size
axes = axes.flatten()  # Flatten in case of a 2D array of axes

for i, df in enumerate(solution[:20]):
    ax = axes[i]

    df.plot(column="centre_dispatcher_evolved",
                                              ax=ax, legend=False)

    # Plot historical boundaries
    grouped_dispatcher_gdf_starting_solution.plot(ax=ax, linewidth=0.5, edgecolor="black")

    ax.set_title(f"Solution {i+1}")
    ax.axis("off")  # Hide axes for better visualization

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()
```

:::{.callout-warning}
Many of thse solutions may perform better against the demand balancing criteria, but suffer from being shapes that are suboptimal in the real world.

We will explore how we may tackle that in future sections. We may choose to prioritise solutions that perform well on measures of [compactness](https://web.stevenson.edu/mbranson/m4tp/version1/gerrymandering-math-topic-compactness.html), for example. A wide range of compactness metrics exist, such as the Polsby-Popper score and the Reock score, among others.

Additional options are the indentation score (convex hull comparison) and the area to perimeter ratio to check for regions with long, thin corridors.

We can also look at measures of connectivity like the edge cut score, which looks at how many border connections would be required to cut off a region's connectivity.


:::: {.columns}

For example, the light blue region on the left here would require very little change to cut it off from itself.

::: {.column width='50%'}

[](assets/2025-03-26-12-31-11.png)


:::

::: {.column width='50%'}

This would be better.

[](assets/2025-03-26-12-33-57.png)

:::

::::

As we bring in additional metrics like real-world travel times into our scoring in subsequent chapters, we may naturally begin to develop 'better' regions.

:::

### Evaluating the solutions against the demand

Let's initially evaluate our evolved solutions based on our original single metric, which is how well each solution balances demand across the regions (minimizing the difference in demand per region, looking at historical demand data).

:::{.callout-note collapse="true"}
### Click here to view our 'evaluate_solution()' function from the previous chapter

```{python}
#| label: define-evaluation-functions
def evaluate_solution(gdf,
                      allocation_column='centre_dispatcher_NEW',
                      demand_column="demand"):
    grouped_by_dispatcher = gdf.groupby(allocation_column)[[demand_column]].sum()
    mean_demand = grouped_by_dispatcher['demand'].mean()

    grouped_by_dispatcher['difference_from_mean'] = (grouped_by_dispatcher['demand'] - mean_demand).astype('int')

    return abs(grouped_by_dispatcher['difference_from_mean']).mean().round(1)

def evaluate_solution_dict(solution_dict, gdf,
                            allocation_column='centre_dispatcher',
                            demand_column="demand", territory_unit_column="LSOA11CD"):

    owned_territory_df = pd.DataFrame(
            [(key, value) for key, values in solution_dict.items() for value in values],
            columns=[f"{allocation_column}_NEW", territory_unit_column])
    gdf = pd.merge(gdf, owned_territory_df, on=territory_unit_column, how="left")

    grouped_by_dispatcher = gdf.groupby(f"{allocation_column}_NEW")[[demand_column]].sum()
    mean_demand = grouped_by_dispatcher['demand'].mean()

    grouped_by_dispatcher['difference_from_mean'] = (grouped_by_dispatcher['demand'] - mean_demand).astype('int')

    return abs(grouped_by_dispatcher['difference_from_mean']).mean().round(1)
```

:::

```{python}
#| label: evaluate-evolved-solutions
import copy

evaluations_evolved_solutions = []

for i, sol in enumerate(solution):
    allocation_evaluation = evaluate_solution(sol.merge(demand, on="LSOA11CD"), allocation_column="centre_dispatcher_evolved")

    evaluations_evolved_solutions.append({
        'solution': f"e{i+1}",
        # We can't just pass the dictionary here due to the way python handles dictionaries
        # We need to explicitly take a copy of the dictionary
        'allocations': copy.deepcopy(sol.groupby('centre_dispatcher_evolved')['LSOA11CD'].apply(list).to_dict()),
        'result': allocation_evaluation
    })

evolution_solution_df = pd.DataFrame(evaluations_evolved_solutions)
evolution_solution_df['rank'] = evolution_solution_df['result'].rank(method='max')

evolution_solution_df.sort_values('rank')
```

Let's add the original random solutions into our ranking table.

```{python}
#| label: add-original-solutions-eval-vals
initial_solution_df['what'] = 'random'
evolution_solution_df['what'] = 'evolved'

full_sol_df = pd.concat([initial_solution_df, evolution_solution_df])
full_sol_df['rank'] = full_sol_df['result'].rank(method='max')
full_sol_df.sort_values('rank')
```

```{python}
#| label: generate-plot-solution-ranking-all
import plotly.express as px

# Sort by 'result' to ensure correct order
full_sol_df = full_sol_df.sort_values("result", ascending=True)

# Convert 'solution' to a string (if it's not already)
full_sol_df["solution"] = full_sol_df["solution"].astype(str)

# Create the plot and enforce sorted order
fig = px.bar(
    full_sol_df,
    x="solution",
    y="result",
    color="what",
    category_orders={"solution": full_sol_df["solution"].tolist()}  # Enforce order
)

fig.show()

```

We will now take a specified portion of these solutions, evolve them, evaluate them again, and continue.

We will also start tracking how good the best solution we are achieving is after each evolution.

We will repeat this 1000 times, plotting the value for our best solution over time to see how much better we can make the solution with sufficient permutations.

:::{.callout-tip}
Coming soon!
:::

Let's plot what we started with versus our best 10 solutions at the end of the process.

Here is our final start-to-finish code for generating and evaluating the solutions.

:::{.callout-tip}
Coming soon!
:::
