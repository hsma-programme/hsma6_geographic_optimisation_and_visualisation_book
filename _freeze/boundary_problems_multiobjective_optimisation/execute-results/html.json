{
  "hash": "b91628b036a6e2e719fe778bdc100746",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"(WIP) Boundary Problems - Evaluating Solutions on Multiple Metrics\"\nformat:\n    html:\n        other-links:\n        - text: Multi-objective optimisation example - hyper-acute stroke unit (HASU) provision\n          href: https://doi.org/10.1136/bmjopen-2017-018143\n        - text: HASU paper supplementary material - more details\n          href: https://github.com/MichaelAllen1966/stroke_unit_location/blob/master/bmj_open_paper/bmj_open_stroke_unit_location_paper_supplementary_material-1.pdf\n        - text: Multi-objective optimisation example - consultant-led childbirth rate maximisation\n          href: https://doi.org/10.1136/bmjopen-2019-034830\n        code-links:\n        - text: HASU Paper Repository (site location)\n          href: https://github.com/MichaelAllen1966/stroke_unit_location\n        - text: Maternal Birth Unit Repository (site location)\n          href: https://github.com/MichaelAllen1966/1901_birth_centre_location\n        - text: Generalised Multi-objective Genetic Algorithm Repository (site location)\n          href: https://github.com/MichaelAllen1966/1807_acute_healthcare_location_effect_of_boundaries\n---\n\n\n\n\nSo far, we have just been evaluating our solutions on a single critiera - the average absolute difference in demand across the region.\n\nHowever, we will often want to evaluate our solutions on multiple metrics.\n\nIn addition, some of these may be more important to us than others, so we may want a way to *weight* the resulting values when considering the overall score of a solution.\n\nFirst, we'll explore building some additional metrics into our current example, and then we will move on to building functions that allow us to evaluate a solution flexibly based on some or all of these metrics.\n\n\n## Defining a score function\n\nLet's start building ourselves an adaptable function that we can build on to gradually add more objectives to.\n\n:::{.callout-note}\nThis approach is inspired by the work undertaken here by [Mike Allen, Kerry Pearn, Emma Villeneuve, Thomas Monks, Ken Stein and Martin James](https://github.com/MichaelAllen1966/stroke_unit_location/blob/master/pyf_ga05_functions_170406.py)\n:::\n\n:::{.callout-tip}\nNote that here we've defined some scores that are specifically applicable to the problem we are targeting. However, the function could be adapted to use as many or as few of these, or as many other scoring criteria, as you wish. Take a look at the example linked above to see how you could bring some other criteria in, such as travel times.\n:::\n\n### Determining whether a solution is better or worse than another solution.\n\nIt's also worth mentioning at this point the criteria that we will be using for determining whether a solution is 'better' than another as we progressively test multiple solutions.\n\nThis is a bit more complex to define than with a single-objective problem, where we could simply define whether a higher or lower score was better in the given context.\n\nInstead, here we will use the idea of 'dominance'.\n\nLet's visualise our scores as a list of numbers, with each number representing the. In this example, we'll assume a higher score is always better.\n\n**Solution 1**: `[1   6   8]`\n\n**Solution 2**: `[2   6   9]`\n\n:::{.callout-note}\n\nThe technical definition of dominance in this context is\n\n> \"A vector `a` of the objective space **dominates** another vector `b` if all criteria of `a` are better or equal to criteria of `b` and `a≠b`\"[^1]\n\nThis simply means that\n\n- the outputs are not identical\n- every score has to be **better than** or *at least equal to* the solution we are comparing against\n\nA solution is non-dominated if\n- the score for one criteria is **better than** at least one score in another solution\n- all remaining scores are at least equal to the other scores\n\nAfter evaluating multiple solutions, you will end up with a series of non-dominated solutions that form the **Pareto Front**.\n\n:::\n\n[^1]: Zhou A, Qu B-Y, Li H, et al. Multiobjective evolutionary algorithms: A survey of the state of the art. Swarm Evol Comput 2011;1:32–49. doi:10.1016/j.swevo.2011.03.001\n\nSo in this case, solution 2 would be better than solution 1 as it is no worse than solution 1 in any respect, and better in some respects.\n\n### Trade-offs\n\nIt's also worth considering that when we start to look at solutions to multi-objective problems, we will generally be unable to find a single 'best' solution that performs optimally across every single objective.\n\nThere will usually be some level of trade-off to be had, with some solutions performing better in one aspect than in others. Generating, evaluating and capturing a wide range of solutions and their scores can help us to start understanding - and visualising - what this trade-off looks like.\n\n\n\n### Code for the scoring function\n\n:::{.callout-tip}\nOptimizing on every score at once can be slow; it's a good idea to build the ability into your function to turn the scoring of objectives on and off.\n:::\n\n::: {#3830fefe .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\ndef score(dispatcher_allocations, pareto_include, weighting=None, calc_all=False):\n    \"\"\"\n    Evaluates a population of proposed boundaries based on multiple performance metrics.\n\n    This function calculates a variety of metrics related to.\n\n    Parameters that are scored on are:\n    1. Total number of calls per dispatcher\n        (minimise DIFFERENCE between total calls per dispatcher across all dispatchers in the solution)\n    2. Maximum number of calls controlled by a single dispatcher\n        (minimise - we want no individual dispatchers to be receiving an unusually low number of calls compared to other dispatchers)\n    3. Minimum number of calls controlled by a single dispatcher\n        (maximise - we want no individual dispatchers to be receiving an unusually high number of calls compared to other dispatchers)\n    4. Total number of resources controlled per dispatcher\n        (minimise DIFFERENCE between the number of resources controlled by each dispatcher in the solution)\n\n    It supports Pareto-based evaluation, computing only the necessary metrics unless CALC_ALL is set to True, in which case all metrics are calculated.\n\n\n    Parameters\n    ----------\n    dispatcher_allocations: dict\n\n    pareto_include: list\n        List of booleans relating to the objectives\n        e.g. evaluating objectives 1 and 4 only would require the list [True, False, False, True]\n\n    weighting: opt, list\n        List of weights for the objectives. Weights should be positive.\n        If None, it will be assumed that all objectives are equally important (have equal weighting).\n        If passed, length must be equal to the number of pareto objectives.\n\n    calc_all: boolean\n        If true, pareto_include will be ignored and all metrics will be used for calculations\n    \"\"\"\n\n    # CODER HERE TO CALCULATE THE\n\n    return (score_matrix, call_matrix, resource_allocation_matrix)\n\n\n\n\ndef normalise_score(score_matrix, norm_matrix):\n    \"\"\"\n    Normalises a 'score matrix' with reference to 'norm matrix' which gives scores that produce zero or one\n\n    Notes\n    -----\n\n    Based on the approach in [GitHub Link](https://github.com/MichaelAllen1966/stroke_unit_location/blob/master/pyf_ga05_functions_170406.py)\n    The referenced code is licensed under Apache 2.0.\n    \"\"\"\n\n    norm_score=np.zeros(np.shape(score_matrix)) # create normlaises score matrix with same dimensions as original scores\n    number_of_scores=len(score_matrix[0,:]) # number of different scores\n    for col in range(number_of_scores): # normaise for each score in turn\n        score_zero=norm_matrix[col,0]\n        score_one=norm_matrix[col,1]\n        score_range=score_one-score_zero\n        norm_score[:,col]=(score_matrix[:,col]-score_zero)/score_range\n    return norm_score\n\ndef pareto(scores):\n    \"\"\"\n    This function takes an array or list of 'scores' and returns a Boolean numpy array identifying which rows of the array 'scores' are non-dominated (the Pareto front).\n\n    Scores should be normalised so that higher values dominate lower values.\n\n    The method is based on assuming everything starts on Pareto front, and then dominated points are recorded\n\n    Parameters\n    ----------\n    scores: list or numpy array\n\n    Returns\n    -------\n    numpy array\n        returns a Boolean numpy array identifying which rows of the array 'scores' are non-dominated (the Pareto front). Dominated scores are identified with a 0.\n\n    Notes\n    -----\n\n    Based on the approach in [GitHub Link](https://github.com/MichaelAllen1966/stroke_unit_location/blob/master/pyf_ga05_functions_170406.py)\n    The referenced code is licensed under Apache 2.0.\n    \"\"\"\n    if isinstance(scores, np.ndarray):\n        pop_size=len(scores[:,0])\n    elif type(scores) == 'list':\n        pop_size=len(scores)\n\n    pareto_front=np.ones(pop_size, dtype=bool)\n\n    for i in range(pop_size):\n        for j in range(pop_size):\n            if all (scores[j]>=scores[i]) and any (scores[j]>scores[i]):\n                # j dominates i\n                pareto_front[i]=0\n                break\n\n    return pareto_front\n```\n:::\n\n\n",
    "supporting": [
      "boundary_problems_multiobjective_optimisation_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}